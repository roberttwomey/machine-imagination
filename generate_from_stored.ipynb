{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generate_from_stored.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "o5YjaXywie10",
        "Vq9sxuzjgBWX",
        "TKvgZRpiuFeB"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/roberttwomey/machine-imagination/blob/main/generate_from_stored.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwTP4MYk0bYn"
      },
      "source": [
        "# BigGAN + CLIP + CMA-ES: Interpolation\n",
        "\n",
        "This notebook generates latent interpolations between different images produced by BigGAN+CLIP+CMA-ES. Given a series of class and noise vectors (each a point in \"latent space\"), we will generate intermediate images and save the results as a video. \n",
        "\n",
        "---\n",
        "\n",
        "This notebook is based off of [j.mp/wanderclip](https://j.mp/wanderclip) by Eyal Gruss [@eyaler](https://twitter.com/eyaler) [eyalgruss.com](https://eyalgruss.com).\n",
        "\n",
        "I've modified it to store noise/class vectors for reuse, and adapted it to run on [nautilus.optiputer.net](https://nautilus.optiputer.net)/z8 (not relevant here, because we're on colab because it's free). robert.twomey@gmail.com"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWmKTmvBg7z5"
      },
      "source": [
        "#@title 1. Setup software libraries (run once)\n",
        "#@markdown This cell installs the software libraries necessary to run our \n",
        "#@markdown text-to-image code on this Colab instance: CUDA, torch, torchvision.\n",
        "\n",
        "#@markdown Run this cell once (press the play button at top left). \n",
        "\n",
        "#@markdown (this takes around 4-5 minutes to run)\n",
        "\n",
        "#@markdown Afterwards, restart the kernel. Select __Runtime -> Restart runtime__\n",
        "#@markdown from the top menu.\n",
        "\n",
        "#@markdown Move on to Step 2 once you have restarted.\n",
        "\n",
        "!pip install ipython-autotime\n",
        "%load_ext autotime\n",
        "\n",
        "# prints out what graphics card we have\n",
        "!nvidia-smi -L\n",
        "\n",
        "import subprocess\n",
        "\n",
        "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
        "print(\"CUDA version:\", CUDA_version)\n",
        "\n",
        "if CUDA_version == \"10.0\":\n",
        "    torch_version_suffix = \"+cu100\"\n",
        "elif CUDA_version == \"10.1\":\n",
        "    torch_version_suffix = \"+cu101\"\n",
        "elif CUDA_version == \"10.2\":\n",
        "    torch_version_suffix = \"\"\n",
        "else:\n",
        "    torch_version_suffix = \"+cu110\"\n",
        "\n",
        "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html ftfy regex"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1SDpkkK7cU1y"
      },
      "source": [
        "#@title 2. Install ML Models\n",
        "#@markdown Installs BigGAN â€” the image generator network. That is all we need\n",
        "#@markdown to create our latent walks. Everything else is already in colab.\n",
        "\n",
        "#@markdown (this takes around 1 minute to run)\n",
        "\n",
        "# BigGAN\n",
        "!pip install pytorch-pretrained-biggan\n",
        "\n",
        "from IPython.display import HTML, clear_output\n",
        "from PIL import Image\n",
        "from IPython.display import Image as JupImage\n",
        "import numpy as np\n",
        "import nltk\n",
        "from scipy.stats import truncnorm\n",
        "\n",
        "# from biggan\n",
        "import torch\n",
        "from pytorch_pretrained_biggan import (BigGAN, one_hot_from_names, truncated_noise_sample,\n",
        "                                       save_as_images, convert_to_images) #, display_in_terminal)\n",
        "import logging\n",
        "logging.basicConfig(level=logging.WARNING)\n",
        "\n",
        "# do we need wordnet?\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# load biggan\n",
        "model = BigGAN.from_pretrained('biggan-deep-512')\n",
        "print(\"loaded bigGAN\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "G7Lddt4462o8"
      },
      "source": [
        "#@title 3. Upload your stored class and noise vectors\n",
        "\n",
        "#@markdown Click on \"Choose Files\" below, and select your ...noise.txt and\n",
        "#@markdown ...class.txt files from before. (For instance \"sunrise through a \n",
        "#@markdown window_1_class.txt\", \"sunrise through a window_1_noise.txt\")\n",
        "\n",
        "#@markdown Upload as many pairs of files as you would like. We will generate \n",
        "#@markdown your latent interpolation (\"latent walk\") from these points in image \n",
        "#@markdown space.\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()\n",
        "\n",
        "# for fn in uploaded.keys():\n",
        "#   print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "#       name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pkAWJOI3---c"
      },
      "source": [
        "# set your prompts and order here (copy the text from above), but do not \n",
        "# include the \"_class.txt\" part or \"_noise.txt\" part. So just the stem of each\n",
        "# phrase. The order matters, and you can repeat.\n",
        "\n",
        "prompts = [\n",
        "    \"a sunrise through a window_1\",\n",
        "    \"a dog sitting on a couch_1\", \n",
        "    \"a cat in a refrigerator_255\"\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOWzPLrBbdxW"
      },
      "source": [
        "#@title 4. Generate a latent walk!\n",
        "\n",
        "#@markdown This cell takes each of the points in latent space (coordinates for\n",
        "#@markdown images, and interpolates between them to create a smoothly flowing\n",
        "#@markdown traversal (\"walk\") through the space of possible images. \n",
        "\n",
        "#@markdown Set the following parameters to shape your output movie. FPS is \n",
        "#@markdown frames per second of the output film. num_steps is how many frames\n",
        "#@markdown between each succesive phrase/image, and len_hold is how many frames\n",
        "#@markdown to pause on each resultant image.\n",
        "\n",
        "# the movie\n",
        "fps = 30 #@param {type: 'number'}\n",
        "\n",
        "# the interpolation\n",
        "num_steps = 90 #@param {type:'number'}\n",
        "len_hold = 30 #@param {type: 'number'}\n",
        "\n",
        "truncation = 1.0\n",
        "\n",
        "interpbase = '/content/'\n",
        "moviefilename = 'interpolation_%s.mp4'\n",
        "\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "from numpy import vstack\n",
        "from numpy.random import randn\n",
        "from numpy.random import randint\n",
        "from numpy import arccos\n",
        "from numpy import clip\n",
        "from numpy import dot\n",
        "from numpy import sin\n",
        "from numpy import linspace\n",
        "from numpy.linalg import norm\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# from\n",
        "# https://discuss.pytorch.org/t/help-regarding-slerp-function-for-generative-model-sampling/32475/4\n",
        "\n",
        "# spherical linear interpolation (slerp)\n",
        "def slerp(val, low, high):\n",
        "    omega = arccos(clip(dot(low/norm(low), high/norm(high)), -1, 1))\n",
        "    so = sin(omega)\n",
        "    if so == 0:\n",
        "        # L'Hopital's rule/LERP\n",
        "        return (1.0-val) * low + val * high\n",
        "    return sin((1.0-val)*omega) / so * low + sin(val*omega) / so * high\n",
        " \n",
        "# uniform interpolation between two points in latent space\n",
        "def interpolate_points(p1, p2, n_steps=10):\n",
        "    # interpolate ratios between the points\n",
        "    ratios = np.linspace(0, 1, num=n_steps)\n",
        "    # linear interpolate vectors\n",
        "    vectors = list()\n",
        "    for ratio in ratios:\n",
        "        v = slerp(ratio, p1, p2)\n",
        "        vectors.append(v)\n",
        "    return np.asarray(vectors)\n",
        "\n",
        "def get_class_file(path, prompt):\n",
        "    print(path+'%s*_class.txt'%prompt)\n",
        "    result = glob.glob(path+'%s*_class.txt'%prompt)\n",
        "    return(result)\n",
        "\n",
        "def get_noise_file(path, prompt):\n",
        "    print(path+'%s*_noise.txt'%prompt)    \n",
        "    result = glob.glob(path+'%s*_noise.txt'%prompt)\n",
        "    return(result)\n",
        "\n",
        "class_filenames = [get_class_file('/content/', prompt)[0] for prompt in prompts]\n",
        "noise_filenames = [get_noise_file('/content/', prompt)[0] for prompt in prompts]\n",
        "\n",
        "# print(class_filenames, noise_filenames)\n",
        "\n",
        "class_inputs = [np.loadtxt(filename) for filename in class_filenames]\n",
        "noise_inputs = [np.loadtxt(filename) for filename in noise_filenames]\n",
        "\n",
        "count = 0\n",
        "\n",
        "# loop over inputs\n",
        "\n",
        "for i in range(len(class_inputs)):\n",
        "\n",
        "    # generate interpolations\n",
        "    noises = interpolate_points(noise_inputs[i], noise_inputs[(i+1)%len(class_inputs)], num_steps)\n",
        "    classes = interpolate_points(class_inputs[i], class_inputs[(i+1)%len(class_inputs)], num_steps)\n",
        "\n",
        "    # generate images in batches\n",
        "    batch_size = 10 # 50\n",
        "    for j in range(0, num_steps, batch_size):\n",
        "        clear_output()\n",
        "        print(i, j, count)\n",
        "        noise_vector = noises[j:j+batch_size]\n",
        "        class_vector = classes[j:j+batch_size]\n",
        "\n",
        "        # convert to tensors\n",
        "        noise_vector = torch.tensor(noise_vector, dtype=torch.float32)\n",
        "        class_vector = torch.tensor(class_vector, dtype=torch.float32)\n",
        "\n",
        "        # put everything on cuda (GPU)\n",
        "        noise_vector = noise_vector.to('cuda')\n",
        "        noise_vector = noise_vector.clamp(-2*truncation, 2*truncation)\n",
        "        class_vector = class_vector.to('cuda')\n",
        "        class_vector = class_vector.softmax(dim=-1)\n",
        "        model.to('cuda')\n",
        "\n",
        "        # generate images\n",
        "        with torch.no_grad():\n",
        "            print(noise_vector.shape)\n",
        "            print(class_vector.shape)\n",
        "            output = model(noise_vector, class_vector, truncation)\n",
        "\n",
        "        # If you have a GPU put back on CPU\n",
        "        output = output.to('cpu')\n",
        "\n",
        "        imgs = convert_to_images(output)\n",
        "\n",
        "        # repeat first image\n",
        "        \n",
        "        if j == 0:\n",
        "            for k in range(len_hold):\n",
        "                imgs[0].save(interpbase+\"/output_%05d.png\" % count)\n",
        "                count = count + 1\n",
        "                \n",
        "        for img in imgs: \n",
        "            img.save(interpbase+\"/output_%05d.png\" % count)\n",
        "            count = count + 1\n",
        "\n",
        "# generate mp4\n",
        "out = moviefilename%fps\n",
        "with open('list.txt','w') as f:\n",
        "  for i in range(count):\n",
        "    print('file %s/output_%05d.png\\n'%(interpbase, i))\n",
        "    f.write('file %s/output_%05d.png\\n'%(interpbase, i))\n",
        "!ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
        "# !echo ffmpeg -r $fps -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
        "        \n",
        "# os.system(\"ffmpeg -r {0} -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r {0} {1} -y\".format(fps, out))\n",
        "\n",
        "# # # rename jpg\n",
        "# # frame = 'frame_%05d.jpg'%(sample_num-1)\n",
        "# # jpg = '%s.jpg'%prompt.replace(\" \", \"_\")\n",
        "# # !cp $frame $jpg\n",
        "# print(\"ffmpeg -r {0} -f concat -safe 0 -i list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r {0} {1} -y\".format(fps, out))\n",
        "  \n",
        "# out = '\"/content/%s_%d.mp4\"'%(prompt, seed)\n",
        "# with open('/content/list.txt','w') as f:\n",
        "#   for i in range(sample_num):\n",
        "#     f.write('file /content/output/frame_%05d.jpg\\n'%i)\n",
        "#   for j in range(int(freeze_secs*fps)):\n",
        "#     f.write('file /content/output/frame_%05d.jpg\\n'%i)\n",
        "# !ffmpeg -r $fps -f concat -safe 0 -i /content/list.txt -c:v libx264 -pix_fmt yuv420p -profile:v baseline -movflags +faststart -r $fps $out -y\n",
        "\n",
        "with open(moviefilename%fps, 'rb') as f:\n",
        "  data_url = \"data:video/mp4;base64,\" + b64encode(f.read()).decode()\n",
        "display(HTML(\"\"\"\n",
        "  <video controls autoplay loop>\n",
        "        <source src=\"%s\" type=\"video/mp4\">\n",
        "  </video>\"\"\" % data_url))\n",
        "\n",
        "# from google.colab import files, output\n",
        "output.eval_js('new Audio(\"https://freesound.org/data/previews/80/80921_1022651-lq.ogg\").play()')\n",
        "files.download(moviefilename%fps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5YjaXywie10"
      },
      "source": [
        "# Explainer\n",
        "\n",
        "There are three parts to this generative system:\n",
        "1. BigGAN is our image generation network\n",
        "2. CLIP is our text-to-image association network (textual descriptions, really)\n",
        "3. CMA-ES is our search/optimizer strategy.\n",
        "\n",
        "### BigGAN\n",
        "\n",
        "BigGAN (https://arxiv.org/abs/1809.11096) is a variety of Generative Adversarial Network (GAN) that set a standard for high resolution, high fidelity image synthesis in 2018. It contained four times as many parameters and eight times the batch size fo previous models, and synthesized a state of the art 512 x 512 pixel image across [1000 different classes](https://gist.githubusercontent.com/yrevar/942d3a0ac09ec9e5eb3a/raw/238f720ff059c1f82f368259d1ca4ffa5dd8f9f5/imagenet1000_clsidx_to_labels.txt) from [Imagenet](https://www.image-net.org/). It was also prohibitively expensive to train! Thankfully Google has released a number of pretrained models for us to explore.\n",
        "\n",
        "BigGAN takes two inputs: a 256-dimensional \"noise\" vector, and a 1000-dimensional one hot \"class\" vector. The \"class\" selects which category of image it is trying to generate (or what mix of categories). The noise vector (latent vector) determines the appearance of this particular instance from within the category (\"dog\" from \"dogs\").\n",
        "\n",
        "You can substitue a different generative network with CLIP to achieve a similar aim. For instance, people are experimenting with [StyleGAN2 ADA](https://colab.research.google.com/drive/1J8xyNRTNVnkNbQJnidcgSdDCHHKfGa8N?usp=sharing#scrollTo=I-YJmx89HLro), [DALL-Es encoder](https://colab.research.google.com/drive/1NGM9L8qP0gwl5z5GAuB_bd0wTNsxqclG), [Lucent's FFT](https://colab.research.google.com/github/eps696/aphantasia/blob/master/Illustra.ipynb). Each generative approach will have different qualities, aesthetics, representational range due to training data and method. Reddit users have produced [a list of CLIP notebooks](https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais/) from this hobbyist/experimenter community.\n",
        "\n",
        "### CLIP\n",
        "OpenAI's CLIP is the key part of this text-to-image translation.\n",
        "\n",
        "[CLIP model card](https://github.com/openai/CLIP/blob/main/model-card.md)\n",
        "\n",
        "### CMA-ES\n",
        "\n",
        "Covariance matrix adaptation evolution strategy (CMA-ES) is a strategy for numerical optimization.\n",
        "\n",
        "This is our strategy for searching what combinations of noise + class vector (BigGAN inputs) produce the best representation of the prompt, according to CLIP (which knows how to relate images and textual descriptions). From a given starting point (random class, random noise), CMA-ES guides the changes in class and noise to improve the output image from BigGAN, to better satisfy CLIP. \n",
        "\n",
        "You can use this same GAN + CLIP architecture with a different optimizer to achieve a similar aim.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vq9sxuzjgBWX"
      },
      "source": [
        "# Activities\n",
        "- Try experimenting with different prompts, but leave the other fields the same.\n",
        "  - Change the prompt and select \"Runtime->restart and run all\" from the top menu. \n",
        "- Try textual prompts of different forms. Instead of \"a photo of\", try \"a drawing of\", \"a picture of\", something else. Or \"a drawing of X, a type of Y\" as mentioned above. \n",
        "- To produce different results with the same prompt, try changing the seed. How do your results change?\n",
        "- Save any results you like. Since we seeded the random value and ran it fresh each time (\"Runtime->restart and run all\") these results should be replicable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TKvgZRpiuFeB"
      },
      "source": [
        "# References\n",
        "\n",
        "- Based on SIREN+CLIP Colabs by: [@advadnoun](https://twitter.com/advadnoun), [@norod78](https://twitter.com/norod78)\n",
        "\n",
        "Using the works:\n",
        "- https://github.com/openai/CLIP\n",
        "- https://tfhub.dev/deepmind/biggan-deep-512\n",
        "- https://github.com/huggingface/pytorch-pretrained-BigGAN\n",
        "- http://www.aiartonline.com/design-2019/eyal-gruss (WanderGAN)\n",
        "- Other CLIP notebooks: https://www.reddit.com/r/MachineLearning/comments/ldc6oc/p_list_of_sitesprogramsprojects_that_use_openais\n",
        "- A curated list of more online generative tools see: [j.mp/generativetools](https://j.mp/generativetools)\n",
        "\n",
        "Other CLIP notebooks:\n",
        "- BigSLEEP (from [@advadnoun](https://twitter.com/advadnoun)): https://colab.research.google.com/drive/1NCceX2mbiKOSlAd_o7IU7nA9UskKN5WR?usp=sharing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdOwVOo8uhsd"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}